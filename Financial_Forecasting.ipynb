{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Credit Risk Prediction with Financial Data \n",
    "\n",
    "This Jupyter Notebook demonstrates the preprocessing of a financial dataset from Kaggle and the prediction of credit risk scores. Two machine learning approaches/algorithms - Logistic Regression and Decision Trees - are applied to forecast credit risk. This project is part of my learning process as a novice in ML/Data Science.\n",
    "Note that the dataset that is used here must be downloaded separately from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Financial_Dataset.csv')  ## load data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the dataset by Id and Quarter because the entries are unordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Company_ID', 'Quarter'], axis=0, ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is more than one row containing the same company for the same quarter, it is recomendable to check duplicates with respect to the columns ID and quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(['Company_ID', 'Quarter'], keep= False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['Company_ID', 'Quarter'], keep='last', inplace = True) ## Now all redundant entries are reduced to the last respective one\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells are intended to get an overview of the data, especially the number of companies, if all four quarters are registered for every company.\n",
    "Moreover we add the Column Quarters_per_Company to later remove all companies that have just one quarter recorded more easy, since that means that those entries are no time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_coms = len(df['Company_ID'].unique())\n",
    "attribute_values = df['Company_ID'].unique()\n",
    "print(number_of_coms)\n",
    "print(attribute_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Company_ID')['Quarter'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.groupby('Company_ID')['Quarter'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_quarter_per_com = df.groupby('Company_ID')['Quarter'].unique().apply(lambda x: x.size)\n",
    "n_quarter_per_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_quarter_per_com.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby('Company_ID')['Quarter'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quarters_per_Company'] = df['Company_ID'].map(counts)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Debt_to_Equity'] == 4.99]['Credit_Risk_Score'] # Interesting to see that all companies that have the maximum value of 4.99 with respect to debt to equity ratio, \n",
    "                                                      # have different credit risk scores, which means that the impact of debt to equity on the risk score is not that big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quarter'] = pd.Categorical(df['Quarter']) # Turning variables of type object to category, because this is the right datatype to work with for vars with finite numbers of characteristics\n",
    "df['Stock_Trend'] = pd.Categorical(df['Stock_Trend'])\n",
    "df['Credit_Risk_Score'] = pd.Categorical(df['Credit_Risk_Score'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stock_Trend_Code'] = df['Stock_Trend'].cat.codes # One Hot Encoding the columns Stock Trend and target column Credit Risk Score from high, medium, low to 0, 1, 2\n",
    "df['Credit_Risk_Score_Code'] = df['Credit_Risk_Score'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few steps contain to put all numerical variables into one list, compute the correlation of them with the target variable to decide   which of them are of interest for the further analysis. Moreover we build moving averages as well as standard deviations and differences of them and add those as columns to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_vars = ['Company_ID', 'Quarter', 'Credit_Risk_Score', 'Stock_Trend']\n",
    "corr_vars = [item for item in df.columns if item not in ex_vars]\n",
    "corr_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[corr_vars].corr(method='pearson')['Credit_Risk_Score_Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in corr_vars[:8]:\n",
    "    df[f'{item}_moving_average'] = df.groupby('Company_ID')[item].transform(lambda x: x.rolling(window=2, min_periods = 1).mean())\n",
    "    df[f'{item}_moving_std'] = df.groupby('Company_ID')[item].transform(lambda x: x.rolling(window=3, min_periods = 1).std()).fillna(0)\n",
    "    df[f'{item}_diff'] = df.groupby('Company_ID')[item].transform(lambda x: x.diff()).fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['Revenue', 'Net_Profit', 'Debt_to_Equity', 'Current_Ratio', 'EPS', 'Stock_Volatility', 'Market_Cap', 'Credit_Score'] # Plotting the value distributions of all numeric columns\n",
    "fig, axs = plt.subplots(2,4, figsize=(16,9), tight_layout = True)\n",
    "axs = axs.flatten()\n",
    "for index, item in enumerate(vars):\n",
    "    axs[index].hist(df[item], bins = 10, color='lightblue', edgecolor = 'black')\n",
    "    axs[index].set_xlabel(f'{item} Value Range')\n",
    "    axs[index].set_ylabel('Frequency')\n",
    "    axs[index].set_title('Variables Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_quarter_per_companys = n_quarter_per_com.to_dict()\n",
    "number_quarter_per_companys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few steps all company ids with all four quarters recorded get filtered, saved in the list companies and then the dataframe gets filtered according to those ids. For our purpose just the first 12 ids are used as representatives. We than plot for every of those companies the original numeric values like revenue and so on accross all 4 quarters to see how values change. Finally we store the mean value and the standard deviation of all those numeric values (list --> vars) in a seperate dictionary, replace outliers in those columns identified by interquartile range with the mean value and check the dictionary again. As we see in the distribution plots the values are very even distributed so no wonder that nothing really happended to the means and std, since no significant outliers were detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = []\n",
    "for index, item in enumerate(number_quarter_per_companys):\n",
    "    if number_quarter_per_companys[item] == 4:\n",
    "        companies.append(item)\n",
    "companies = companies[:12]\n",
    "filterd_df = df[df['Company_ID'].isin(companies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filterd_df.set_index(['Company_ID', 'Quarter'],drop=False)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['Revenue', 'EPS', 'Debt_to_Equity', 'Net_Profit', 'Market_Cap', 'Stock_Volatility']\n",
    "fig, axs = plt.subplots(12, 6, figsize= (24, 20), tight_layout=True)\n",
    "for index, item in enumerate(companies):\n",
    "    for id, it in enumerate(vars):\n",
    "        axs[index][id].plot(filtered_df[filtered_df['Company_ID']==item]['Quarter'], filtered_df[filtered_df['Company_ID']==item][it])\n",
    "        axs[index][id].set_xlabel('Quarters')\n",
    "        axs[index][id].set_ylabel(it)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means, stds = {}, {}\n",
    "for item in vars:\n",
    "    mean = df[item].mean()\n",
    "    std = df[item].std()\n",
    "    means[item] = mean\n",
    "    stds[item] = std\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in vars:\n",
    "    mean = df[item].mean()\n",
    "    std = df[item].std()\n",
    "    df.loc[(df[item] > mean+3*std) | (df[item] < mean-3*std)] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in vars:\n",
    "    mean = df[item].mean()\n",
    "    std = df[item].std()\n",
    "    means[item] = mean\n",
    "    stds[item] = std\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Quarters_per_Company'] > 1] # remove all entries/companys with just one quarter recorded\n",
    "df.set_index(['Company_ID', 'Quarter'], drop=False, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps collect all unique companies/ids and split them into ids used for training and those that are going to used for testing.\n",
    "According to the ids we can then build up the training and the test sets. This is crucial since we have time series data and at least two entries for every company, which means the same id could be moved to training and testing set if we gave the whole X and y to the train_test_split from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df.index.get_level_values(level=0).unique() \n",
    "ids = np.array(ids)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id, test_id = train_test_split(ids, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[df['Company_ID'].isin(train_id)].drop(columns=['Company_ID','Credit_Risk_Score', 'Stock_Trend', 'Quarters_per_Company', 'Credit_Risk_Score_Code'], axis=1)\n",
    "X_test = df[df['Company_ID'].isin(test_id)].drop(columns=['Company_ID', 'Credit_Risk_Score', 'Stock_Trend', 'Quarters_per_Company', 'Credit_Risk_Score_Code'], axis=1)\n",
    "y_train = df[df['Company_ID'].isin(train_id)]['Credit_Risk_Score_Code']\n",
    "y_test = df[df['Company_ID'].isin(test_id)]['Credit_Risk_Score_Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape) # the shapes of X and y sets look promising\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we scale the values of all numeric columns by StandardScaler and then train the algorithms (Logistic Regression and Decision Tree) to finally predict our credit risk scores. Here I also decided to first remove 'Quarter' from the X_train and X_test. We then use all important classification metrics to consider our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.drop(columns=['Quarter'], axis=1))\n",
    "X_test = scaler.transform(X_test.drop(columns=['Quarter'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(multi_class='multinomial', class_weight='balanced')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(X_test)\n",
    "results['truth'] = test_y\n",
    "results['Predictions'] = preds\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_test, preds, labels=[0,1,2], average='weighted'))\n",
    "print(recall_score(y_test, preds, labels=[0,1,2], average='weighted'))\n",
    "print(precision_score(y_test, preds, labels=[0,1,2], average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, preds)\n",
    "sns.heatmap(cm, annot=True, cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "parameter_grid = {'max_depth': [3, 5, 7, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [2, 3, 4, 5]}\n",
    "grid_search = GridSearchCV(model, parameter_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "mod = grid_search.best_estimator_\n",
    "score_predictions = mod.predict(X_test)\n",
    "score_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_test, score_predictions, average='weighted'))\n",
    "print(recall_score(y_test, score_predictions, average='weighted'))\n",
    "print(precision_score(y_test, score_predictions, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, score_predictions)\n",
    "sns.heatmap(cm, annot=True, cmap='bwr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da_cr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
